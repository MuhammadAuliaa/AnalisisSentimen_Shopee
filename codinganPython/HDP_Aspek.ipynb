{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sisi Koding\n",
    "1. Rapihin preprocessingnya\n",
    "2. Jalankan HDP untuk liat top 30 nya (baru setelah ini bisa kabarin aku dulu biar aku liat list topiknya dan aku bisa tentuin aspeknya (kelarin HDP) \n",
    "3. Topik yang aku pilih akan menjadi tolak ukur untuk Analisa sentimennya. Sekaligus merapihan data-data yang akan di mapping ke aspek tersebut \n",
    "mau, baik, bahan, warna, kain.\n",
    "warna ukuran bahan = \n",
    "\n",
    "4. Analisa sentiment berbasis aspeknya menggunakan SVM dan Indobert -> hasil akhir mirip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interface :\n",
    "svm & indobert -> harga berapa positif & negatif, bahan berapa positif & negatif, warna berapa positif & negatif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "program bisa diakses tanpa coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 menu : bisa masukin link\n",
    "\n",
    "- crawling data (sentimen negatif 50 positif 50 (rate1-3 50%, rate 4-5 50%))\n",
    "- segmentasi kain (lembut, halus, dll) -> (3-5 aspek : warna, bahan, kualitas)\n",
    "- barchart kosongin jika gada sesuai dengan aspek (hasil ulasan tidak mengandung tema bahan, warna, dan kain!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  catatan :\n",
    "1. segment -> aspek\n",
    "2. bar -> merge (merah -> negatif, biru -> positif) -> bar nempel\n",
    "3. mencari coherence terbaik yang memuat 3 segement/aspek\n",
    "4. ### HDP (reference codingan & perhitungan) -> screenshoot hasil hdp\n",
    "5. konfigurasi path : HDP, interface (dashboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference HDP : \n",
    "1. https://github.com/bab2min/tomotopy\n",
    "2. https://mlg.eng.cam.ac.uk/zoubin/tut06/ywt.pdf\n",
    "3. https://link.springer.com/article/10.1007/s10994-016-5621-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models import HdpModel\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Muhammad Ade\n",
      "[nltk_data]     Aulia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Muhammad Ade\n",
      "[nltk_data]     Aulia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Muhammad Ade\n",
      "[nltk_data]     Aulia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kata-Kata Berdasarkan Segmentasi:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Segmentasi</th>\n",
       "      <th>Kata</th>\n",
       "      <th>Bobot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bahan</td>\n",
       "      <td>kasar</td>\n",
       "      <td>0.005036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>harga</td>\n",
       "      <td>murah</td>\n",
       "      <td>0.004019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kualitas</td>\n",
       "      <td>style</td>\n",
       "      <td>0.005482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kualitas</td>\n",
       "      <td>kecil</td>\n",
       "      <td>0.004091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kualitas</td>\n",
       "      <td>sederhana</td>\n",
       "      <td>0.004029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>pengiriman</td>\n",
       "      <td>kurir</td>\n",
       "      <td>0.006247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>pengiriman</td>\n",
       "      <td>packingan</td>\n",
       "      <td>0.005161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>respon</td>\n",
       "      <td>banget</td>\n",
       "      <td>0.004220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>warna</td>\n",
       "      <td>pink</td>\n",
       "      <td>0.005438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>warna</td>\n",
       "      <td>warna</td>\n",
       "      <td>0.004467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Segmentasi       Kata     Bobot\n",
       "0        bahan      kasar  0.005036\n",
       "1        harga      murah  0.004019\n",
       "5     kualitas      style  0.005482\n",
       "3     kualitas      kecil  0.004091\n",
       "4     kualitas  sederhana  0.004029\n",
       "..         ...        ...       ...\n",
       "93  pengiriman      kurir  0.006247\n",
       "94  pengiriman  packingan  0.005161\n",
       "95      respon     banget  0.004220\n",
       "96       warna       pink  0.005438\n",
       "97       warna      warna  0.004467\n",
       "\n",
       "[98 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unduh stopwords dan wordnet jika belum diunduh\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Baca data ulasan dari file CSV\n",
    "df = pd.read_csv(\"data/dataHasilPreprocessing/dataPreprocessing.csv\")\n",
    "\n",
    "# Mengisi nilai NaN dengan string kosong\n",
    "df['Ulasan'] = df['Ulasan'].fillna('')\n",
    "\n",
    "# Inisialisasi stop words dan lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Fungsi preprocessing\n",
    "def preprocess(text):\n",
    "    # Tokenisasi\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Hapus stop words dan tanda baca, lakukan lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "    return tokens\n",
    "\n",
    "# Preprocessing data ulasan\n",
    "df['processed'] = df['Ulasan'].apply(preprocess)\n",
    "\n",
    "# Buat dictionary dan corpus\n",
    "dictionary = corpora.Dictionary(df['processed'])\n",
    "corpus = [dictionary.doc2bow(text) for text in df['processed']]\n",
    "\n",
    "# Latih model HDP\n",
    "hdp = HdpModel(corpus, dictionary)\n",
    "\n",
    "# Ekstrak topik dan kata kunci\n",
    "top_topics = hdp.show_topics(num_topics=10, num_words=10, formatted=False)\n",
    "top_words = []\n",
    "\n",
    "# Daftar kata kunci untuk setiap segmentasi\n",
    "segmentation_keywords = {\n",
    "    'bahan': ['tipis', 'tebal', 'lembut', 'keras', 'kasar', 'rapih', 'rapi', 'pendek', 'adem', 'nyaman', 'jahit', 'halus', 'gerah', 'relaxing', 'baju', 'model', 'celana', 'nama', 'transparan', 'badan', 'sayap'],\n",
    "    'pengiriman': ['cepat', 'lambat', 'lelet', 'ontime', 'terlambat', 'instan', 'kurir', 'ekspektasi', 'semangat', 'kilogram', 'packingan', 'super', 'gampang', 'kilo', 'ekspedisi', 'online', 'kirim', 'diiket', 'angkat', 'realita'],\n",
    "    'kualitas': ['rusak', 'sesuai', 'bagus', 'jelek', 'berkualitas', 'keringat', 'sobek', 'aneh', 'foto', 'gambar', 'keren', 'mantap', 'kecil', 'label', 'ngetat', 'ketat', 'pict', 'fashion', 'bolong', 'style', 'sederhana'],\n",
    "    'warna': ['cerah', 'pudar', 'gelap', 'putih', 'hitam', 'warna', 'biru', 'soft', 'navy', 'pink'],\n",
    "    'harga': ['murah', 'mahal', 'terjangkau', 'ekonomis', 'premium', 'uang', 'duit', 'refund', 'promo', 'promonya'],\n",
    "    'respon': ['ragu', 'tidak puas', 'kurang', 'positif', 'negatif', 'astaga', 'tengkyuu', 'emang', 'tanggap', 'nyoba', 'suka', 'worth', 'haha', 'tolong', 'banget', 'balas', 'thanks', 'thank', 'aduh']\n",
    "}\n",
    "\n",
    "# Fungsi untuk menentukan segmentasi\n",
    "def categorize_word(word):\n",
    "    for segment, keywords in segmentation_keywords.items():\n",
    "        if word in keywords:\n",
    "            return segment\n",
    "    return 'other'\n",
    "\n",
    "# Mengelompokkan kata berdasarkan segmentasi\n",
    "segmentation_words = []\n",
    "\n",
    "for i, topic in top_topics:\n",
    "    for word, weight in topic:\n",
    "        segment = categorize_word(word)\n",
    "        segmentation_words.append({'Segmentasi': segment, 'Kata': word, 'Bobot': weight})\n",
    "\n",
    "# Buat DataFrame untuk kata\n",
    "# a-kata berdasarkan segmentasi\n",
    "segmentation_words_df = pd.DataFrame(segmentation_words)\n",
    "segmentation_summary_df = segmentation_words_df.groupby(['Segmentasi', 'Kata']).agg({'Bobot': 'max'}).reset_index()\n",
    "segmentation_summary_df = segmentation_summary_df.sort_values(by=['Segmentasi', 'Bobot'], ascending=[True, False])\n",
    "\n",
    "print(\"\\nKata-Kata Berdasarkan Segmentasi:\")\n",
    "segmentation_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nilai Coherence: 0.7851410845906054\n"
     ]
    }
   ],
   "source": [
    "# Hitung nilai coherence\n",
    "coherence_model = CoherenceModel(model=hdp, texts=df['processed'].tolist(), dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print(f\"\\nNilai Coherence: {coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_load",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
